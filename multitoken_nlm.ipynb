{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/plison/mambaforge/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log probability: -2.7288677991600707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 20:15:45.350769: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-25 20:15:46.931706: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best replacement so far: <mask> Darwin\n",
      "best replacement so far: Charles Darwin\n",
      "Alternative results: {'Charles Darwin': -2.7288677991600707, 'William Thomson': -4.342340171337128, 'George Burgess': -5.420966029167175, 'John Smith': -5.676791429519653, 'Edward Smith': -5.809417009353638, 'John Taylor': -5.944948792457581, 'Thomas Hardy': -6.007612705230713, 'John Bates': -6.015552520751953, 'William Hudson': -6.075858116149902, 'Edward Robinson': -6.120614171028137, 'William Smith': -6.148982286453247, 'William Bates': -6.241352081298828, 'James Burgess': -6.290881037712097, 'George Smith': -6.3188393115997314, 'James Thomson': -6.3247315883636475, 'Joseph Smith': -6.367221117019653, 'Edward Edwards': -6.403763771057129, 'Edward Fisher': -6.4369553327560425, 'John Brown': -6.451346755027771, 'Henry Hudson': -6.464964866638184}\n"
     ]
    }
   ],
   "source": [
    "import torch, transformers\n",
    "import re\n",
    "\n",
    "class MultiTokenMaskLM:\n",
    "    \n",
    "    def __init__(self, model_name=\"roberta-large\"):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = transformers.AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "    def get_log_probability(self, text, text_to_mask):\n",
    "        \"\"\"Returns the log probability of a span included in a larger text\"\"\"\n",
    "        \n",
    "        # We create two tokenized texts: the original one, and the masked one\n",
    "        orig_tokens, masked_tokens = self._tokenize_and_mask(text, text_to_mask)\n",
    "      \n",
    "        # We search for the token indices that have been masked  \n",
    "        mask_ids = [i for i in range(len(orig_tokens[\"input_ids\"][0])) \n",
    "                    if masked_tokens[\"input_ids\"][0,i]==self.tokenizer.mask_token_id]    \n",
    "        \n",
    "        # We compute the log probability token by token, starting with the\n",
    "        # last token (which is often the family name)\n",
    "        total_log_prob = 0\n",
    "        with torch.no_grad():\n",
    "            for mask_id in mask_ids[::-1]:\n",
    "                logits = self.model(**masked_tokens).logits\n",
    "                log_probs = torch.nn.functional.log_softmax(logits[0,mask_id], dim=0)\n",
    "                actual_id = orig_tokens[\"input_ids\"][0,mask_id]\n",
    "                total_log_prob += log_probs[actual_id].item()\n",
    "                \n",
    "                # Once we are done, we replace the mask with the actual token\n",
    "                masked_tokens[\"input_ids\"][0, mask_id] = actual_id\n",
    "                \n",
    "        return total_log_prob\n",
    "        \n",
    "            \n",
    "    def get_alternatives(self, text, text_to_mask, beam_size=20):\n",
    "        \"\"\"Returns a list of possible replacements for a span included in a larger text. \n",
    "        The method relies on a  form of beam search\"\"\"\n",
    "        \n",
    "        # We tokenize and mask the text\n",
    "        _, tokens = self._tokenize_and_mask(text, text_to_mask)\n",
    "\n",
    "        # We search for the token indices that have been masked  \n",
    "        mask_ids = [i for i in range(len(tokens[\"input_ids\"][0])) \n",
    "                    if tokens[\"input_ids\"][0,i]==self.tokenizer.mask_token_id]\n",
    "            \n",
    "        beam = [(tokens,0)] \n",
    "        \n",
    "        # We search for alternatives token by token\n",
    "        for mask_id in mask_ids[::-1]:\n",
    "            new_beam = []  \n",
    "            for current, current_logprob in beam:           \n",
    "                for filled, new_logprob in self._fill(current, mask_id, beam_size):\n",
    "                    new_beam.append((filled, current_logprob + new_logprob))\n",
    "            \n",
    "            # We restrict the beam to a maximum size\n",
    "            beam = sorted(new_beam, key=lambda x : x[1])[-beam_size:]\n",
    "            print(\"best replacement so far:\", self.tokenizer.decode(beam[-1][0][\"input_ids\"][0,mask_ids]))\n",
    "        \n",
    "        # We finally convert the results into strings\n",
    "        beam_string = {self.tokenizer.decode(solution[\"input_ids\"][0, mask_ids]):logprob \n",
    "                       for solution, logprob in beam[::-1]}\n",
    "        \n",
    "        return beam_string      \n",
    "\n",
    "\n",
    "    def _tokenize_and_mask(self, text, text_to_mask):\n",
    "        \"\"\"Returns two tokenized representations of the text: the original one,\n",
    "        and one where all tokens included in the text span to mask are replaced\n",
    "        by a special <mask> value.\"\"\"\n",
    "        \n",
    "        if text_to_mask is not None and text_to_mask not in text:\n",
    "            raise RuntimeError(\"Text to mask must be included in full text\")\n",
    "\n",
    "        # We run the tokenizer (with offset mapping to find the tokens to mask)\n",
    "        orig_tokens = self.tokenizer(text, return_offsets_mapping=True, return_tensors=\"pt\")\n",
    "        offset_mapping = orig_tokens[\"offset_mapping\"][0]\n",
    "        del orig_tokens[\"offset_mapping\"]\n",
    "        \n",
    "    #    print(\"tokens:\", [self.tokenizer.decode(x) for x in orig_tokens[\"input_ids\"][0]])\n",
    "        \n",
    "        # We create the masked version\n",
    "        masked_tokens = {key:value.clone().detach() for key, value in orig_tokens.items()}\n",
    "        for match in re.finditer(re.escape(text_to_mask), text):\n",
    "            for i, (tok_start, tok_end) in enumerate(offset_mapping):\n",
    "                if tok_start >= match.start(0) and tok_end <= match.end(0) and tok_end > tok_start:\n",
    "                    masked_tokens[\"input_ids\"][0,i] = self.tokenizer.mask_token_id\n",
    "        \n",
    "        return orig_tokens, masked_tokens\n",
    "           \n",
    "           \n",
    "    def _fill(self, tokens, mask_id, beam_size=100):\n",
    "        \"\"\"Generates possible updated list of tokens where the masked token is replaced.\n",
    "        Each candidate is associated with a given log-probability\"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**tokens).logits\n",
    "            log_probs = torch.nn.functional.log_softmax(logits[0,mask_id], dim=0)\n",
    "\n",
    "            best_replacements_idx = torch.argsort(log_probs)[-beam_size:]    \n",
    "            \n",
    "            for replacement_id in best_replacements_idx:\n",
    "                new_tokens = {key:value.clone().detach() for key, value in tokens.items()}\n",
    "                new_tokens[\"input_ids\"][0, mask_id] = replacement_id\n",
    "                logprob = log_probs[replacement_id].item()\n",
    "                yield new_tokens, logprob\n",
    "\n",
    "        \n",
    "filler = MultiTokenMaskLM(model_name=\"roberta-large\")\n",
    "\n",
    "text = \"\"\"Charles Darwin (12 February 1809 â€“ 19 April 1882) was an English naturalist, geologist, and biologist, widely known for contributing to the understanding of evolutionary biology.\"\"\"\n",
    "print(\"Log probability:\", filler.get_log_probability(text, \"Charles Darwin\"))\n",
    "print(\"Alternative results:\", filler.get_alternatives(text, \"Charles Darwin\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8cf70501ca3f55aeaffff5fddbd1cd8fbcfc2281300f723b9fb0f393b77fe62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
